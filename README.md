# Go API MongoDB Scrapper

[![CI](https://github.com/maxime-louis14/go_api_mongo_scrapper/workflows/Continuous%20Integration/badge.svg)](https://github.com/maxime-louis14/go_api_mongo_scrapper/actions/workflows/ci.yml)
[![CD](https://github.com/maxime-louis14/go_api_mongo_scrapper/workflows/Continuous%20Deployment/badge.svg)](https://github.com/maxime-louis14/go_api_mongo_scrapper/actions/workflows/cd.yml)
[![Release](https://github.com/maxime-louis14/go_api_mongo_scrapper/workflows/Release/badge.svg)](https://github.com/maxime-louis14/go_api_mongo_scrapper/actions/workflows/release.yml)
[![Go Report Card](https://goreportcard.com/badge/github.com/maxime-louis14/go_api_mongo_scrapper)](https://goreportcard.com/report/github.com/maxime-louis14/go_api_mongo_scrapper)

Une API REST en Go avec MongoDB et un scraper de recettes performant utilisant des goroutines.

## Fonctionnalit√©s

- **API REST** : Serveur web avec Fiber framework
- **Base de donn√©es** : MongoDB avec Docker
- **Scraper performant** : Scraping parall√®le avec goroutines
- **Tests complets** : Tests unitaires avec couverture de code
- **CI/CD automatis√©** : Pipeline GitHub Actions
- **Docker** : Containerisation compl√®te
- **Cross-platform** : Binaires pour Linux, Windows, macOS

## Architecture

```
‚îú‚îÄ‚îÄ controllers/     # Contr√¥leurs API
‚îú‚îÄ‚îÄ database/       # Configuration MongoDB
‚îú‚îÄ‚îÄ models/         # Mod√®les de donn√©es
‚îú‚îÄ‚îÄ routes/         # Routes API
‚îú‚îÄ‚îÄ responses/      # R√©ponses API
‚îú‚îÄ‚îÄ scraper/        # Module de scraping
‚îÇ   ‚îú‚îÄ‚îÄ scraper.go     # Code principal
‚îÇ   ‚îú‚îÄ‚îÄ scraper_test.go # Tests unitaires
‚îÇ   ‚îî‚îÄ‚îÄ README_TESTS.md # Documentation tests
‚îú‚îÄ‚îÄ docs/           # Documentation
‚îú‚îÄ‚îÄ .github/        # Workflows CI/CD
‚îî‚îÄ‚îÄ Makefile        # Commandes de build
```

## Installation

### Pr√©requis

- Go 1.20+
- Docker & Docker Compose
- Make (optionnel)

### Installation rapide

```bash
# Cloner le repository
git clone https://github.com/maxime-louis14/go_api_mongo_scrapper.git
cd go_api_mongo_scrapper

# Installer les d√©pendances
go mod download

# D√©marrer MongoDB avec Docker
docker-compose up -d

# Lancer l'API
go run main.go
```

### Avec Make

```bash
# Installation compl√®te
make deps

# Lancer les tests
make test

# Compiler
make build-all

# Lancer l'API
make run-server
```

## Utilisation

### API REST

L'API est disponible sur `http://localhost:8080`

```bash
# V√©rifier l'√©tat de l'API
curl http://localhost:8080/health

# Endpoints disponibles
GET    /recipes          # Liste des recettes
POST   /recipes          # Cr√©er une recette
GET    /recipes/:id      # R√©cup√©rer une recette
PUT    /recipes/:id      # Modifier une recette
DELETE /recipes/:id      # Supprimer une recette
```

### Scraper

```bash
# Lancer le scraper
cd scraper
go run scraper.go

# Ou avec Make
make run
```

Le scraper utilise des goroutines pour un scraping parall√®le performant :
- 10 workers simultan√©s
- Protection contre les race conditions
- Statistiques en temps r√©el
- Gestion d'erreurs robuste

## Tests

### Ex√©cution des tests

```bash
# Tests unitaires
make test

# Tests avec race detection
make test-verbose

# Rapport de couverture
make test-coverage

# Benchmarks
make benchmark
```

### Couverture de code

Le projet maintient une couverture de tests de **22.6%** avec :
- 12 tests unitaires
- 2 benchmarks
- Tests de concurrence
- Tests de validation

Voir [README_TESTS.md](scraper/README_TESTS.md) pour plus de d√©tails.

## CI/CD Pipeline

Le projet utilise GitHub Actions pour l'automatisation :

### Continuous Integration (CI)

D√©clench√© sur push/PR vers `main` et `develop` :

- **Code Quality** : Formatage, linting, analyse statique
- **Tests** : Tests unitaires avec race detection
- **Build** : Compilation cross-platform
- **Security** : Scan de s√©curit√© avec Gosec
- **Docker** : Build et test des images

### Continuous Deployment (CD)

- **Staging** : D√©ploiement automatique sur push vers `main`
- **Production** : D√©ploiement sur tags `v*`
- **Rollback** : Rollback automatique en cas d'√©chec

### Release

Cr√©ation automatique de releases avec :
- Binaires multi-plateformes
- Images Docker multi-architecture
- Changelog automatique
- Assets GitHub

Voir [docs/CICD.md](docs/CICD.md) pour la documentation compl√®te.

## Docker

### Images disponibles

```bash
# Derni√®re version
docker pull ghcr.io/maxime-louis14/go_api_mongo_scrapper:latest

# Version sp√©cifique
docker pull ghcr.io/maxime-louis14/go_api_mongo_scrapper:v1.0.0
```

### Utilisation

```bash
# Avec Docker Compose (recommand√©)
docker-compose up

# Ou manuellement
docker run -p 8080:8080 ghcr.io/maxime-louis14/go_api_mongo_scrapper:latest
```

## D√©veloppement

### Commandes Make

```bash
make help              # Afficher l'aide
make ci                # Pipeline CI local
make ci-full           # CI avec couverture et benchmarks
make docker-build      # Construire l'image Docker
make release VERSION=v1.0.0  # Cr√©er une release
```

### Workflow de d√©veloppement

1. **Cr√©er une branche feature**
   ```bash
   git checkout -b feature/nouvelle-fonctionnalite
   ```

2. **D√©velopper et tester**
   ```bash
   make test
   make ci
   ```

3. **Cr√©er une Pull Request**
   - Le CI s'ex√©cute automatiquement
   - Tous les checks doivent passer

4. **Merge vers main**
   - D√©ploiement automatique en staging

5. **Cr√©er une release**
   ```bash
   git tag v1.0.0
   git push origin v1.0.0
   ```

## Performance

### Scraper

- **Parall√©lisme** : 10 goroutines simultan√©es
- **Vitesse** : ~64 recettes en 3.2 secondes
- **M√©moire** : Optimis√© avec channels et sync.Pool
- **Robustesse** : Gestion d'erreurs et timeouts

### API

- **Framework** : Fiber (Express-like pour Go)
- **Base de donn√©es** : MongoDB avec indexation
- **Middleware** : CORS, logging, compression
- **Performance** : ~10k req/s en conditions optimales

## Monitoring

### M√©triques disponibles

- **Health check** : `/health` endpoint
- **Logs** : Structured logging avec niveaux
- **M√©triques** : Temps de r√©ponse, erreurs, throughput

### Alertes

- **CI/CD** : Notifications automatiques sur √©checs
- **Dependabot** : Alertes de s√©curit√©
- **GitHub Security** : Scan de vuln√©rabilit√©s

## Contribution

1. Fork le projet
2. Cr√©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

### Standards de code

- **Formatage** : `gofmt` obligatoire
- **Linting** : `golangci-lint` sans erreurs
- **Tests** : Couverture minimale de 80%
- **Documentation** : Commentaires Go standard

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## Support

- **Issues** : [GitHub Issues](https://github.com/maxime-louis14/go_api_mongo_scrapper/issues)
- **Discussions** : [GitHub Discussions](https://github.com/maxime-louis14/go_api_mongo_scrapper/discussions)
- **Documentation** : [docs/](docs/)

## Roadmap

- [ ] Authentification JWT
- [ ] Rate limiting
- [ ] Cache Redis
- [ ] M√©triques Prometheus
- [ ] Dashboard Grafana
- [ ] Tests d'int√©gration E2E
- [ ] D√©ploiement Kubernetes

## Deployment

Pour activer le scraper fait.
```bash
  sudo docker compose up -d --build
```


# Bonjour, je suis Maxime ! Voici un projet demand√© par mon √©cole NWS üëã

## Consignes

Le restaurant H√≥twings souhaite d√©velopper son activit√© avec de la vente en livraison. Le restaurant mise sur ce nouveau service de la fa√ßon suivante: une carte tr√®s √©tendue.

Pour plaire √† tous les go√ªts, le restaurant vous a demand√© de d√©velopper une API permettant de proposer beaucoup de plats et de recettes.

Vous allez devoir concevoir cette API, mais aussi devoir l'alimenter ! Le restaurant aimerait que vous r√©cup√©riez les recettes depuis le site https://www.allrecipes.com/

Dans un soucis de benchmark, vous avez promis au client d'impl√©menter 2 bases de donn√©es diff√©rentes. Vous devez concevoir votre API avec une base de donn√©es SQL et NoSQL. Votre API doit pouvoir fonctionner avec n'importe quelle base de donn√©es, l'une sans l'autre.

Afin de vous assurer du fonctionnement de votre produit, vous veillerez √† ce qu'un Swagger soit mis en place.

Votre scrapper sera capable de g√©n√©rer un fichier JSON contenant toutes les informations scrapp√©es. Une route sur votre API vous permettra d'importer les nouvelles donn√©es dans la base de donn√©es choisie par l'utilisateur.

## Fonctionnalit√©s attendues

### Fonctionnalit√©s de Lecture

- Lister les recettes ‚áí get
- Lister une recette, ses ingr√©dients et ses √©tapes de pr√©paration ‚áí get

### Fonctionnalit√© de Recherche

- Rechercher une recette par nom
- Rechercher une recette par ingr√©dient

### Importation de la base de donn√©es

- Importer la base de donn√©es depuis un fichier JSON dans la base de donn√©es choisie

### Outils & Stack

Voici la stack qui vous est **recommand√©e** pour le projet:

- MySQL / MariaDB pour le SQL
- MongoDB pour le NoSQL

## üîó Links

Vous pouvez retrouver l'API API_golang_Mysql et le scrapper_go

[![Golang scrapper_go](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/le-veilleur/scrapper_go)
[![API_golang_Mysql](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/le-veilleur/go_api__scrapper_mysql_docker)


